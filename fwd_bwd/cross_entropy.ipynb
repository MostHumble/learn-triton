{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "from triton.runtime import driver\n",
        "import tabulate"
      ],
      "metadata": {
        "id": "0jyolDB_g-h7"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Mv6nDjK7w_yH"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def cross_entropy_forward_kernel(\n",
        "    logits_ptr,  # [B, C]\n",
        "    targets_ptr,  # [B] (int)\n",
        "    loss_ptr,     # [B]\n",
        "    stride_batch,\n",
        "    stride_class,\n",
        "    num_classes,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    row = tl.program_id(0)  # index over batch\n",
        "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
        "    mask = col_offsets < num_classes\n",
        "\n",
        "    # pointer to start of row\n",
        "    row_ptr = logits_ptr + row * stride_batch\n",
        "\n",
        "    # load logits\n",
        "    logits = tl.load(row_ptr + col_offsets * stride_class, mask=mask, other=-float('inf'))\n",
        "\n",
        "    # numerical stability trick\n",
        "    max_logit = tl.max(logits, axis=0)\n",
        "    logits = logits - max_logit\n",
        "\n",
        "    exp_logits = tl.exp(logits)\n",
        "    sum_exp = tl.sum(exp_logits, axis=0)\n",
        "    log_sum_exp = tl.log(sum_exp)\n",
        "\n",
        "    target_idx = tl.load(targets_ptr + row)\n",
        "    true_logit = tl.load(row_ptr + target_idx * stride_class)\n",
        "\n",
        "    loss = log_sum_exp - (true_logit - max_logit)\n",
        "    tl.store(loss_ptr + row, loss)"
      ],
      "metadata": {
        "id": "DT_G02GVhzAm"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def cross_entropy_backward_kernel(\n",
        "    logits_ptr,\n",
        "    targets_ptr,\n",
        "    grad_loss_ptr,\n",
        "    grad_logits_ptr,\n",
        "    stride_batch,\n",
        "    stride_class,\n",
        "    num_classes,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    row = tl.program_id(0)\n",
        "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
        "    mask = col_offsets < num_classes\n",
        "\n",
        "    row_ptr = logits_ptr + row * stride_batch\n",
        "\n",
        "    logits = tl.load(row_ptr + col_offsets * stride_class, mask=mask, other=-float('inf'))\n",
        "    max_logit = tl.max(logits, axis=0)\n",
        "    logits = logits - max_logit\n",
        "\n",
        "    exp_logits = tl.exp(logits)\n",
        "    sum_exp = tl.sum(exp_logits, axis=0)\n",
        "    softmax = exp_logits / sum_exp\n",
        "\n",
        "    target_idx = tl.load(targets_ptr + row)\n",
        "    grad_loss = tl.load(grad_loss_ptr + row)\n",
        "    grad = softmax\n",
        "    grad = tl.where(col_offsets == target_idx, grad - 1.0, grad)\n",
        "    grad = grad * grad_loss  # chain rule\n",
        "\n",
        "    out_ptr = grad_logits_ptr + row * stride_batch\n",
        "    tl.store(out_ptr + col_offsets * stride_class, grad, mask=mask)"
      ],
      "metadata": {
        "id": "9uyzTuvW7-1D"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropyTriton(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, logits, targets):\n",
        "        B, C = logits.shape\n",
        "        logits_ = logits.contiguous()\n",
        "        targets_ = targets.contiguous()\n",
        "\n",
        "        loss = torch.empty(B, device=logits.device, dtype=logits.dtype)\n",
        "        BLOCK_SIZE = triton.next_power_of_2(C)\n",
        "\n",
        "        grid = lambda meta: (B,)\n",
        "\n",
        "        cross_entropy_forward_kernel[grid](\n",
        "            logits_,\n",
        "            targets_,\n",
        "            loss,\n",
        "            logits_.stride(0),\n",
        "            logits_.stride(1),\n",
        "            C,\n",
        "            BLOCK_SIZE=BLOCK_SIZE,\n",
        "        )\n",
        "\n",
        "        ctx.save_for_backward(logits_, targets_, loss)\n",
        "        return loss\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        logits, targets, loss = ctx.saved_tensors\n",
        "        B, C = logits.shape\n",
        "        grad_logits = torch.empty_like(logits)\n",
        "        BLOCK_SIZE = triton.next_power_of_2(C)\n",
        "\n",
        "        grid = lambda meta: (B,)\n",
        "\n",
        "        cross_entropy_backward_kernel[grid](\n",
        "            logits,\n",
        "            targets,\n",
        "            grad_output.contiguous(),\n",
        "            grad_logits,\n",
        "            logits.stride(0),\n",
        "            logits.stride(1),\n",
        "            C,\n",
        "            BLOCK_SIZE=BLOCK_SIZE,\n",
        "        )\n",
        "\n",
        "        return grad_logits, None  # no grad wrt targets\n"
      ],
      "metadata": {
        "id": "-EInW4JI8q90"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TritonCrossEntropyLoss(torch.nn.Module):\n",
        "    def forward(self, logits, targets):\n",
        "        return CrossEntropyTriton.apply(logits, targets).mean()"
      ],
      "metadata": {
        "id": "mixsHU9cQqIC"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = torch.randn(4, 10, device='cuda', requires_grad=True)\n",
        "targets = torch.randint(0, 10, (4,), device='cuda')\n",
        "\n",
        "loss_fn = TritonCrossEntropyLoss()\n",
        "loss = loss_fn(logits, targets)\n",
        "loss.backward()\n",
        "\n",
        "# Compare to PyTorch\n",
        "logits_ref = logits.detach().clone().requires_grad_()\n",
        "loss_ref = torch.nn.functional.cross_entropy(logits_ref, targets)\n",
        "loss_ref.backward()\n",
        "\n",
        "loss_diff = abs(loss.item() - loss_ref.item())\n",
        "grad_diff = (logits.grad - logits_ref.grad).abs().max().item()\n",
        "\n",
        "\n",
        "print(\"Loss diff:\", loss_diff)\n",
        "print(\"Grad diff (max):\", grad_diff)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRP7fABB8szD",
        "outputId": "d1e9be95-bd2c-45fd-e992-410409b3f560"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss diff: 0.0\n",
            "Grad diff (max): 7.450580596923828e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More efficient:"
      ],
      "metadata": {
        "id": "UUcO2J-vF-0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reuses the logsumexp, takes into account padding"
      ],
      "metadata": {
        "id": "o71GPhtMGBJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def cross_entropy_forward_kernel(\n",
        "    logits_ptr,  # [B, C]\n",
        "    targets_ptr,  # [B] (int)\n",
        "    loss_ptr,     # [B]\n",
        "    logsumexp_ptr, # [B] - for saving\n",
        "    stride_batch,\n",
        "    stride_class,\n",
        "    num_classes,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    row = tl.program_id(0)\n",
        "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
        "    mask = col_offsets < num_classes\n",
        "\n",
        "    row_ptr = logits_ptr + row * stride_batch\n",
        "    logsumexp_out_ptr = logsumexp_ptr + row\n",
        "    loss_out_ptr = loss_ptr + row\n",
        "\n",
        "    logits = tl.load(row_ptr + col_offsets * stride_class, mask=mask, other=-float('inf'))\n",
        "    logits = logits.to(tl.float32)\n",
        "\n",
        "    max_logit = tl.max(logits, axis=0)\n",
        "    max_logit_safe = tl.where(max_logit == -float('inf'), 0.0, max_logit)\n",
        "    shifted_logits = logits - max_logit_safe\n",
        "    exp_logits = tl.exp(shifted_logits)\n",
        "    sum_exp = tl.sum(exp_logits, axis=0)\n",
        "    logsumexp_val = max_logit_safe + tl.log(sum_exp)\n",
        "    tl.store(logsumexp_out_ptr, logsumexp_val)\n",
        "\n",
        "    target_idx = tl.load(targets_ptr + row)\n",
        "    true_logit = tl.load(row_ptr + target_idx * stride_class)\n",
        "    true_logit = true_logit.to(tl.float32)\n",
        "\n",
        "    loss = logsumexp_val - true_logit\n",
        "    tl.store(loss_out_ptr, tl.where(target_idx != -100, loss, 0.0))\n",
        "\n",
        "@triton.jit\n",
        "def cross_entropy_backward_kernel(\n",
        "    logits_ptr,\n",
        "    targets_ptr,\n",
        "    grad_loss_ptr,\n",
        "    grad_logits_ptr,\n",
        "    logsumexp_ptr,\n",
        "    stride_batch,\n",
        "    stride_class,\n",
        "    num_classes,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    row = tl.program_id(0)\n",
        "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
        "    mask = col_offsets < num_classes\n",
        "\n",
        "    row_ptr = logits_ptr + row * stride_batch\n",
        "    logits = tl.load(row_ptr + col_offsets * stride_class, mask=mask, other=-float('inf'))\n",
        "    logits = logits.to(tl.float32)\n",
        "\n",
        "    logsumexp = tl.load(logsumexp_ptr + row)\n",
        "    softmax = tl.exp(logits - logsumexp)\n",
        "\n",
        "    target_idx = tl.load(targets_ptr + row)\n",
        "    grad_loss = tl.load(grad_loss_ptr + row)\n",
        "\n",
        "    grad = softmax\n",
        "    grad = tl.where(col_offsets == target_idx, grad - 1.0, grad)\n",
        "    grad = grad * tl.where(target_idx != -100, grad_loss, 0.0)\n",
        "\n",
        "    out_ptr = grad_logits_ptr + row * stride_batch\n",
        "    tl.store(out_ptr + col_offsets * stride_class, grad, mask=mask)\n",
        "\n",
        "class CrossEntropyTriton(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, logits, targets):\n",
        "        B, C = logits.shape\n",
        "        logits_ = logits.contiguous()\n",
        "        targets_ = targets.contiguous()\n",
        "\n",
        "        loss = torch.empty(B, device=logits.device, dtype=logits.dtype)\n",
        "        logsumexp = torch.empty_like(loss)\n",
        "        BLOCK_SIZE = triton.next_power_of_2(C)\n",
        "\n",
        "        grid = lambda meta: (B,)\n",
        "\n",
        "        cross_entropy_forward_kernel[grid](\n",
        "            logits_,\n",
        "            targets_,\n",
        "            loss,\n",
        "            logsumexp,\n",
        "            logits_.stride(0),\n",
        "            logits_.stride(1),\n",
        "            C,\n",
        "            BLOCK_SIZE=BLOCK_SIZE,\n",
        "        )\n",
        "\n",
        "        ctx.save_for_backward(logits_, targets_, logsumexp)\n",
        "        return loss\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        logits, targets, logsumexp = ctx.saved_tensors\n",
        "        B, C = logits.shape\n",
        "        grad_logits = torch.empty_like(logits)\n",
        "        BLOCK_SIZE = triton.next_power_of_2(C)\n",
        "\n",
        "        grid = lambda meta: (B,)\n",
        "\n",
        "        cross_entropy_backward_kernel[grid](\n",
        "            logits,\n",
        "            targets,\n",
        "            grad_output.contiguous(),\n",
        "            grad_logits,\n",
        "            logsumexp,\n",
        "            logits.stride(0),\n",
        "            logits.stride(1),\n",
        "            C,\n",
        "            BLOCK_SIZE=BLOCK_SIZE,\n",
        "        )\n",
        "\n",
        "        return grad_logits, None\n",
        "\n",
        "class TritonCrossEntropyLoss(torch.nn.Module):\n",
        "    def forward(self, logits, targets):\n",
        "        return CrossEntropyTriton.apply(logits, targets).mean()\n",
        "\n",
        "logits = torch.randn(4, 10, device='cuda', requires_grad=True)\n",
        "targets = torch.randint(0, 10, (4,), device='cuda')\n",
        "\n",
        "loss_fn = TritonCrossEntropyLoss()\n",
        "loss = loss_fn(logits, targets)\n",
        "loss.backward()\n",
        "\n",
        "logits_ref = logits.detach().clone().requires_grad_()\n",
        "loss_ref = torch.nn.functional.cross_entropy(logits_ref, targets)\n",
        "loss_ref.backward()\n",
        "\n",
        "loss_diff = abs(loss.item() - loss_ref.item())\n",
        "grad_diff = (logits.grad - logits_ref.grad).abs().max().item()\n",
        "\n",
        "print(\"Loss diff:\", loss_diff)\n",
        "print(\"Grad diff (max):\", grad_diff)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-Tkl6igy3sF",
        "outputId": "b9a4a4a9-69b5-48a9-b7e8-1ad58b12ff4d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss diff: 0.0\n",
            "Grad diff (max): 1.4901161193847656e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8yuGaAd5FLyQ"
      },
      "execution_count": 36,
      "outputs": []
    }
  ]
}
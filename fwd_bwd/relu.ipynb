{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "from triton.runtime import driver\n",
        "import tabulate"
      ],
      "metadata": {
        "id": "0jyolDB_g-h7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Mv6nDjK7w_yH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def relu_forward_kernel(\n",
        "    x_ptr, y_ptr, n_elements,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    pid = tl.program_id(0)\n",
        "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "    mask = offsets < n_elements\n",
        "\n",
        "    x = tl.load(x_ptr + offsets, mask=mask)\n",
        "    y = tl.where(x > 0, x, 0.0)\n",
        "    tl.store(y_ptr + offsets, y, mask=mask)"
      ],
      "metadata": {
        "id": "DT_G02GVhzAm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def relu_backward_kernel(\n",
        "    x_ptr, grad_out_ptr, grad_in_ptr, n_elements,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    pid = tl.program_id(0)\n",
        "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "    mask = offsets < n_elements\n",
        "\n",
        "    x = tl.load(x_ptr + offsets, mask=mask)\n",
        "    grad_out = tl.load(grad_out_ptr + offsets, mask=mask)\n",
        "    # propagate gradients if the input was positive\n",
        "    grad_in = tl.where(x > 0, grad_out, 0.0)\n",
        "    tl.store(grad_in_ptr + offsets, grad_in, mask=mask)\n"
      ],
      "metadata": {
        "id": "9uyzTuvW7-1D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Function\n",
        "\n",
        "class ReLU_Triton(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        # triton expects flat array\n",
        "        x_flat = x.contiguous().view(-1)\n",
        "        y = torch.empty_like(x_flat)\n",
        "        BLOCK_SIZE = 1024\n",
        "        n_elements = x_flat.numel()\n",
        "\n",
        "        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
        "\n",
        "        relu_forward_kernel[grid](x_flat, y, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n",
        "\n",
        "        ctx.save_for_backward(x_flat)\n",
        "        return y.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_out):\n",
        "        (x_flat,) = ctx.saved_tensors\n",
        "        grad_out_flat = grad_out.contiguous().view(-1)\n",
        "        grad_in = torch.empty_like(x_flat)\n",
        "        BLOCK_SIZE = 1024\n",
        "        n_elements = x_flat.numel()\n",
        "\n",
        "        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
        "\n",
        "        relu_backward_kernel[grid](x_flat, grad_out_flat, grad_in, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n",
        "\n",
        "        return grad_in.view_as(x_flat)\n",
        "\n",
        "\n",
        "class TritonReLU(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return ReLU_Triton.apply(x)\n"
      ],
      "metadata": {
        "id": "-EInW4JI8q90"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shortcut\n",
        "relu = TritonReLU()\n",
        "x = torch.randn(2, device='cuda', requires_grad=True)\n",
        "y = relu(x)\n",
        "loss = y.sum()\n",
        "loss.backward()\n",
        "\n",
        "# Check:\n",
        "print(\"x:\", x)\n",
        "print(\"y = relu(x):\", y)\n",
        "print(\"x.grad (should be 1 where x > 0):\", x.grad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRP7fABB8szD",
        "outputId": "899afb1b-e005-4401-c3ad-caf0417def7d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: tensor([-0.6888,  2.0654], device='cuda:0', requires_grad=True)\n",
            "y = relu(x): tensor([0.0000, 2.0654], device='cuda:0', grad_fn=<ReLU_TritonBackward>)\n",
            "x.grad (should be 1 where x > 0): tensor([0., 1.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "igtxAEVt83OK"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}